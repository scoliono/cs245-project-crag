{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rSz4rkJWKOu"
   },
   "source": [
    "# First-time setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4Iv0A3WNoFU"
   },
   "source": [
    "Clone the project base repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cs5LyUF3H_C1",
    "outputId": "e4f88ccf-e845-4e31-e543-8972a2def63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'cs245-project-crag' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yecchen/cs245-project-crag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaNRlkuXRiez"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!cd /content/cs245-project-crag && pip install -r requirements.txt\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHjB-4C78tgm",
    "outputId": "9e2522a2-4a8e-4d2a-9139-11006d3ee568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"...\"\n",
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qobViAWSIxX",
    "outputId": "ab44271f-fee4-46aa-9281-8b0b79d0d8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 706M\n",
      "-rw-r--r-- 1 root root 706M Nov 26 09:13 crag_task_1_dev_v4_release.jsonl.bz2\n",
      "drwxr-xr-x 2 root root 4.0K Nov 26 09:16 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# CRAG dataset files located under /home/data\n",
    "#!cd /home/data && bzip2 crag_task_1_dev_v4_release.jsonl\n",
    "#!ls -lAh /home/data\n",
    "!ls -lAh /content/cs245-project-crag/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt0rLZqSYgxr"
   },
   "outputs": [],
   "source": [
    "# import starter code\n",
    "import sys\n",
    "sys.path.append('/content/cs245-project-crag/course_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLraCm-MWQ-4"
   },
   "source": [
    "# Vanilla/RAG baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wd__pWJToR6T",
    "outputId": "977ddd54-c30c-4bea-bd19-35d1e071f1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Kills existing service\n",
    "!pkill -f serve\n",
    "# Use 'nohup' and & operator to start the inference server in the background\n",
    "# Note: can also try meta-llama/Llama-3.2-3B-Instruct with --max_model_len=2048, except one of the RAG contexts is 2493 so it will eventually fail\n",
    "!nohup vllm serve unsloth/Llama-3.2-3B-Instruct-bnb-4bit --gpu_memory_utilization=0.95 --tensor_parallel_size=1 --dtype=\"half\" \\\n",
    "  --quantization=\"bitsandbytes\" --load-format=\"bitsandbytes\" --port=8088 --enforce_eager --max_model_len=4096 &\n",
    "# Give it time to load the model\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOOhLoAOs1qv",
    "outputId": "ef0b026d-2b63-4ea5-fe18-12fb05889ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2024-12-02 02:46:46.940929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-02 02:46:46.963895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-02 02:46:46.970651: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 02:46:46.986892: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 02:46:48.203048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1335it [24:24,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python generate.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --split 1 \\\n",
    "  --model_name \"vanilla_baseline\" \\\n",
    "  --llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --is_server \\\n",
    "  --vllm_server \"http://localhost:8088/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yT5ON5fzWID",
    "outputId": "60b0dc1c-6b54-49e8-cd18-0185e78d8ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2024-12-02 03:11:20.488214: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-02 03:11:20.511796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-02 03:11:20.518895: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 03:11:20.539545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 03:11:21.579120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 0it [00:00, ?it/s]/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-02 03:11:30,709\tINFO worker.py:1819 -- Started a local Ray instance.\n",
      "\u001b[36m(_extract_chunks pid=219201)\u001b[0m 2024-12-02 03:11:40.515287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=219201)\u001b[0m 2024-12-02 03:11:40.581043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=219203)\u001b[0m 2024-12-02 03:11:40.578648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=219201)\u001b[0m 2024-12-02 03:11:43.493099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 237it [14:34,  4.29s/it]\u001b[36m(_extract_chunks pid=222182)\u001b[0m 2024-12-02 03:26:09.612051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_extract_chunks pid=222182)\u001b[0m 2024-12-02 03:26:09.637360: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_extract_chunks pid=222182)\u001b[0m 2024-12-02 03:26:09.645021: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_extract_chunks pid=219204)\u001b[0m 2024-12-02 03:11:43.491515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Generating predictions: 241it [15:01,  4.79s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 03:26:30,709 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(_extract_chunks pid=222182)\u001b[0m 2024-12-02 03:26:11.246702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 291it [17:59,  4.34s/it]\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-02 03:29:30,714 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 292it [18:14,  7.49s/it]\u001b[36m(_extract_chunks pid=222921)\u001b[0m 2024-12-02 03:29:49.215252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=222921)\u001b[0m 2024-12-02 03:29:49.239555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=222921)\u001b[0m 2024-12-02 03:29:49.246271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=222921)\u001b[0m 2024-12-02 03:29:50.714606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 311it [19:38,  4.18s/it]\u001b[36m(_extract_chunks pid=223313)\u001b[0m 2024-12-02 03:31:10.770920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=223313)\u001b[0m 2024-12-02 03:31:10.797512: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=223313)\u001b[0m 2024-12-02 03:31:10.805259: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=223313)\u001b[0m 2024-12-02 03:31:12.600781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 315it [20:01,  4.72s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 03:31:30,716 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 344it [21:51,  5.00s/it]\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-02 03:33:30,720 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 345it [22:06,  7.96s/it]\u001b[36m(_extract_chunks pid=223863)\u001b[0m 2024-12-02 03:33:39.843233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=223863)\u001b[0m 2024-12-02 03:33:39.865871: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=223863)\u001b[0m 2024-12-02 03:33:39.872538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=223863)\u001b[0m 2024-12-02 03:33:41.790144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 608it [37:54,  5.91s/it]\u001b[36m(_extract_chunks pid=226856)\u001b[0m 2024-12-02 03:49:29.233301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=226856)\u001b[0m 2024-12-02 03:49:29.261546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=226856)\u001b[0m 2024-12-02 03:49:29.269260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-02 03:49:30,742 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(_extract_chunks pid=226856)\u001b[0m 2024-12-02 03:49:30.768259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 694it [43:37,  4.26s/it]\u001b[36m(_extract_chunks pid=227980)\u001b[0m 2024-12-02 03:55:11.407116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=227980)\u001b[0m 2024-12-02 03:55:11.430503: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=227980)\u001b[0m 2024-12-02 03:55:11.437469: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=227980)\u001b[0m 2024-12-02 03:55:12.972827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 698it [44:00,  4.55s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 03:55:30,750 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 717it [45:17,  5.00s/it]\u001b[36m(_extract_chunks pid=228350)\u001b[0m 2024-12-02 03:56:50.540946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=228350)\u001b[0m 2024-12-02 03:56:50.563622: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=228350)\u001b[0m 2024-12-02 03:56:50.571668: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=228350)\u001b[0m 2024-12-02 03:56:52.391071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 726it [46:00,  4.19s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 03:57:30,754 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 754it [47:41,  3.79s/it]\u001b[36m(_extract_chunks pid=228878)\u001b[0m 2024-12-02 03:59:14.665234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=228878)\u001b[0m 2024-12-02 03:59:14.688643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=228878)\u001b[0m 2024-12-02 03:59:14.695715: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=228878)\u001b[0m 2024-12-02 03:59:16.694373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 757it [48:02,  5.08s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 03:59:30,758 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 816it [51:30,  3.13s/it]\u001b[36m(_extract_chunks pid=229665)\u001b[0m 2024-12-02 04:03:04.501876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=229665)\u001b[0m 2024-12-02 04:03:04.527668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=229665)\u001b[0m 2024-12-02 04:03:04.534850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=229665)\u001b[0m 2024-12-02 04:03:06.015387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 822it [52:02,  4.52s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:03:30,765 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 868it [54:42,  4.79s/it]\u001b[36m(_extract_chunks pid=228878)\u001b[0m /content/cs245-project-crag/course_code/rag_baseline.py:57: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\u001b[36m(_extract_chunks pid=228878)\u001b[0m   soup = BeautifulSoup(html_source, \"lxml\")\n",
      "Generating predictions: 887it [55:57,  4.36s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:07:30,773 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(_extract_chunks pid=230573)\u001b[0m 2024-12-02 04:07:31.745943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=230573)\u001b[0m 2024-12-02 04:07:31.770862: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=230573)\u001b[0m 2024-12-02 04:07:31.777949: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=230573)\u001b[0m 2024-12-02 04:07:33.202590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 928it [58:39,  4.28s/it]\u001b[36m(_extract_chunks pid=231180)\u001b[0m 2024-12-02 04:10:12.369950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231180)\u001b[0m 2024-12-02 04:10:12.396281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231180)\u001b[0m 2024-12-02 04:10:12.403643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231180)\u001b[0m 2024-12-02 04:10:14.236203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 931it [59:00,  5.50s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:10:30,779 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 945it [59:58,  4.55s/it]\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-02 04:11:30,781 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 946it [1:00:04,  4.80s/it]\u001b[36m(_extract_chunks pid=231559)\u001b[0m 2024-12-02 04:11:37.696853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231559)\u001b[0m 2024-12-02 04:11:37.719678: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231559)\u001b[0m 2024-12-02 04:11:37.726640: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231559)\u001b[0m 2024-12-02 04:11:39.188165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 961it [1:01:14,  4.44s/it]\u001b[36m(_extract_chunks pid=231886)\u001b[0m 2024-12-02 04:12:47.181840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231886)\u001b[0m 2024-12-02 04:12:47.204333: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231886)\u001b[0m 2024-12-02 04:12:47.211089: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=231886)\u001b[0m 2024-12-02 04:12:48.563832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 971it [1:02:00,  3.44s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:13:30,784 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 979it [1:02:31,  4.57s/it]\u001b[36m(_extract_chunks pid=232198)\u001b[0m 2024-12-02 04:14:04.279219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=232198)\u001b[0m 2024-12-02 04:14:04.302108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=232198)\u001b[0m 2024-12-02 04:14:04.308917: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=232198)\u001b[0m 2024-12-02 04:14:06.141896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 984it [1:03:00,  4.65s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:14:30,786 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 1007it [1:04:31,  4.03s/it]\u001b[36m(_extract_chunks pid=232669)\u001b[0m 2024-12-02 04:16:05.040259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=232669)\u001b[0m 2024-12-02 04:16:05.062281: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=232669)\u001b[0m 2024-12-02 04:16:05.070929: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=232669)\u001b[0m 2024-12-02 04:16:06.597729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1012it [1:05:01,  5.02s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:16:30,788 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 1132it [1:12:16,  4.83s/it]\u001b[36m(_extract_chunks pid=234279)\u001b[0m 2024-12-02 04:23:49.987434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=234279)\u001b[0m 2024-12-02 04:23:50.014430: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=234279)\u001b[0m 2024-12-02 04:23:50.021282: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=234279)\u001b[0m 2024-12-02 04:23:51.409654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1141it [1:13:01,  3.61s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:24:30,800 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 1209it [1:17:01,  4.66s/it]\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-02 04:28:30,807 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(_extract_chunks pid=235314)\u001b[0m 2024-12-02 04:28:34.010128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=235314)\u001b[0m 2024-12-02 04:28:34.034042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=235314)\u001b[0m 2024-12-02 04:28:34.041733: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=235314)\u001b[0m 2024-12-02 04:28:35.724209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1241it [1:18:58,  3.29s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:30:30,811 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 1242it [1:19:03,  3.66s/it]\u001b[36m(_extract_chunks pid=235812)\u001b[0m 2024-12-02 04:30:36.367167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=235812)\u001b[0m 2024-12-02 04:30:36.390193: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=235812)\u001b[0m 2024-12-02 04:30:36.397010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=235812)\u001b[0m 2024-12-02 04:30:37.956253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1289it [1:21:57,  3.85s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:33:30,819 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 1290it [1:22:05,  5.12s/it]\u001b[36m(_extract_chunks pid=236492)\u001b[0m 2024-12-02 04:33:38.576910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=236492)\u001b[0m 2024-12-02 04:33:38.600409: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=236492)\u001b[0m 2024-12-02 04:33:38.608048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=236492)\u001b[0m 2024-12-02 04:33:39.920679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1332it [1:24:43,  3.73s/it]\u001b[36m(_extract_chunks pid=237120)\u001b[0m 2024-12-02 04:36:17.072807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=237120)\u001b[0m 2024-12-02 04:36:17.096009: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=237120)\u001b[0m 2024-12-02 04:36:17.102876: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=237120)\u001b[0m 2024-12-02 04:36:18.841914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 1334it [1:25:01,  5.91s/it]\u001b[33m(raylet)\u001b[0m [2024-12-02 04:36:30,825 E 219132 219132] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0999ac2e0745816eaec366be11a92798aae62251881b56af0a84d3f8, IP: 172.17.0.2) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.17.0.2`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Generating predictions: 1335it [1:25:05,  3.82s/it]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python generate.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --split 1 \\\n",
    "  --model_name \"rag_baseline\" \\\n",
    "  --llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --is_server \\\n",
    "  --vllm_server \"http://localhost:8088/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4rybOEuzaXa",
    "outputId": "f510332a-9bfe-488a-ec1a-6efa7ecb5016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 04:57:36.979261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-02 04:57:37.002343: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-02 04:57:37.009605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 04:57:37.025798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 04:57:38.255472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "True\n",
      "100% 1335/1335 [09:23<00:00,  2.37it/s]\n",
      "\u001b[32m2024-12-02 05:07:03.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_predictions\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m{'score': 0.038202247191011285, 'exact_accuracy': 0.000749063670411985, 'accuracy': 0.15355805243445692, 'hallucination': 0.11535580524344569, 'missing': 0.7310861423220973, 'n_miss': 976, 'n_correct': 205, 'n_correct_exact': 1, 'total': 1335}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python evaluate.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --model_name \"vanilla_baseline\" \\\n",
    "  --llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --eval_llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --is_server \\\n",
    "  --vllm_server \"http://localhost:8088/v1\" \\\n",
    "  --max_retries 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxZtu7DiyQ5u",
    "outputId": "e56574bd-9eed-4b66-c8a7-ab055af3fc76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-02 05:07:09.163830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-02 05:07:09.185749: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-02 05:07:09.192528: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 05:07:09.208906: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 05:07:10.341752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "True\n",
      "100% 1335/1335 [18:40<00:00,  1.19it/s]\n",
      "\u001b[32m2024-12-02 05:25:53.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_predictions\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m{'score': 0.06666666666666665, 'exact_accuracy': 0.02696629213483146, 'accuracy': 0.30337078651685395, 'hallucination': 0.23670411985018727, 'missing': 0.4599250936329588, 'n_miss': 614, 'n_correct': 405, 'n_correct_exact': 36, 'total': 1335}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python evaluate.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --model_name \"rag_baseline\" \\\n",
    "  --llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --eval_llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --is_server \\\n",
    "  --vllm_server \"http://localhost:8088/v1\" \\\n",
    "  --max_retries 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNY-a8M7WWO4"
   },
   "source": [
    "# RetRobust baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKWcUmv41Ve1",
    "outputId": "2c2d36a6-a9e4-4656-c147-c0ab4f31af4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rFetching 8 files:   0% 0/8 [00:00<?, ?it/s]\rFetching 8 files: 100% 8/8 [00:00<00:00, 25516.68it/s]\n",
      "/root/.cache/huggingface/hub/models--Ori--llama-2-13b-peft-nq-retrobust/snapshots/4160729bf70221a1ba1e35d9c92b4a717830a75a\n",
      "fatal: destination path 'FastChat' already exists and is not an empty directory.\n",
      "Cloning into 'reasoning-on-cots'...\n",
      "remote: Enumerating objects: 195, done.\u001b[K\n",
      "remote: Counting objects: 100% (195/195), done.\u001b[K\n",
      "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
      "remote: Total 195 (delta 65), reused 120 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (195/195), 2.12 MiB | 22.43 MiB/s, done.\n",
      "Resolving deltas: 100% (65/65), done.\n"
     ]
    }
   ],
   "source": [
    "# First-time setup: download LoRA\n",
    "!huggingface-cli download Ori/llama-2-13b-peft-nq-retrobust\n",
    "# First-time setup: download Q4_K_M quantized Llama-2-13B GGUF for approximate performance, float16 won't fit on our GPU\n",
    "# NOTE: GGUF does not seem to work with LoRA on vLLM\n",
    "#!curl -Lo llama-2-13b-chat.Q4_K_M.gguf 'https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_K_M.gguf?download=true'\n",
    "\n",
    "!git clone https://github.com/scoliono/FastChat\n",
    "!git clone https://github.com/oriyor/reasoning-on-cots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkXhIM6YsKIO",
    "outputId": "9e387865-21d9-4d23-c884-3c1c89fd21c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/FastChat\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.10.5)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.115.5)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.27.2)\n",
      "Requirement already satisfied: markdown2[all] in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.5.1)\n",
      "Requirement already satisfied: nh3 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.2.18)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (1.26.4)\n",
      "Requirement already satisfied: prompt_toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (3.0.47)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.9.2)\n",
      "Collecting pydantic-settings (from fschat==0.2.36)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (5.9.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (2.32.3)\n",
      "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (13.8.1)\n",
      "Requirement already satisfied: shortuuid in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (1.0.13)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.7.0)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from fschat==0.2.36) (0.32.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit>=3.0.0->fschat==0.2.36) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.36) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat==0.2.36) (2.18.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->fschat==0.2.36) (4.0.3)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->fschat==0.2.36) (0.41.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->fschat==0.2.36) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->fschat==0.2.36) (0.14.0)\n",
      "Requirement already satisfied: wavedrom in /usr/local/lib/python3.10/dist-packages (from markdown2[all]->fschat==0.2.36) (2.0.3.post3)\n",
      "Requirement already satisfied: latex2mathml in /usr/local/lib/python3.10/dist-packages (from markdown2[all]->fschat==0.2.36) (3.77.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings->fschat==0.2.36) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.36) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fschat==0.2.36) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->fschat==0.2.36) (2024.9.11)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->fschat==0.2.36) (8.1.7)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.36) (0.1.2)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->fschat==0.2.36) (1.2.2)\n",
      "Requirement already satisfied: svgwrite in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.16.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (6.0.2)\n",
      "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Building wheels for collected packages: fschat\n",
      "  Building editable for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fschat: filename=fschat-0.2.36-0.editable-py3-none-any.whl size=15011 sha256=b89807c6141efb6d43d0ccadfda22f90b5837058e4aa062c0c548ee3ada71884\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dr91pxpz/wheels/0e/7d/3f/6256e4d259fdebc8665545ea33ca3112027cecb15f3a295311\n",
      "Successfully built fschat\n",
      "Installing collected packages: pydantic-settings, fschat\n",
      "  Attempting uninstall: fschat\n",
      "    Found existing installation: fschat 0.2.36\n",
      "    Uninstalling fschat-0.2.36:\n",
      "      Successfully uninstalled fschat-0.2.36\n",
      "Successfully installed fschat-0.2.36 pydantic-settings-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!cd FastChat && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvRfwijGJzh9",
    "outputId": "c75c87a4-40e9-4676-bc7d-d371998d9938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: appending output to 'nohup.out'\n",
      "nohup: appending output to 'nohup.out'\n",
      "nohup: appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Kills existing service\n",
    "!pkill -f fastchat\n",
    "\n",
    "# Use 'nohup' and & operator to start the inference server in the background\n",
    "\n",
    "#! vllm serve meta-llama/Llama-2-13b-chat-hf --load-format=bitsandbytes --quantization=bitsandbytes \\\n",
    "#  --kv-cache-dtype=fp8\n",
    "#  --enable-lora --lora-modules='retrobust=/root/.cache/huggingface/hub/models--Ori--llama-2-13b-peft-nq-retrobust/snapshots/4160729bf70221a1ba1e35d9c92b4a717830a75a' \\\n",
    "#  --gpu_memory_utilization=0.95 --tensor_parallel_size=1 --port=8088 --enforce_eager &\n",
    "\n",
    "# AWQ 4bit quantized model can fit on a 16GB GPU\n",
    "#!nohup vllm serve TheBloke/Llama-2-13B-chat-GPTQ --tokenizer meta-llama/Llama-2-13b-chat-hf \\\n",
    "#--enable-lora --lora-modules='retrobust=/root/.cache/huggingface/hub/models--Ori--llama-2-13b-peft-nq-retrobust/snapshots/4160729bf70221a1ba1e35d9c92b4a717830a75a' \\\n",
    "#--gpu_memory_utilization=0.95 --tensor_parallel_size=1 --port=8088 --enforce_eager &\n",
    "# Give it time to load the model\n",
    "#!sleep 30\n",
    "\n",
    "!export FASTCHAT_WORKER_API_EMBEDDING_BATCH_SIZE=1\n",
    "!nohup python3 -m fastchat.serve.controller --host=0.0.0.0 &\n",
    "!sleep 3\n",
    "!nohup python3 -m fastchat.serve.model_worker --host=0.0.0.0 --model-name \"text-davinci-003\" --model-path \"meta-llama/Llama-2-13b-hf\" --load-8bit &\n",
    "!sleep 200\n",
    "!nohup python3 -m fastchat.serve.openai_api_server --host=0.0.0.0 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-q6qxvQidl6",
    "outputId": "e6caade6-3875-4f6e-f80d-ec6c510e1597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 0.28.0\n",
      "Uninstalling openai-0.28.0:\n",
      "  Successfully uninstalled openai-0.28.0\n",
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=1fec672a7b9deb023ee28a04217365ef914a8f56ec252a553c64e78ad209a93a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia, openai\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.6.0 requires openai>=1.0, but you have openai 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# import starter code\n",
    "import sys\n",
    "sys.path.append('/content/reasoning-on-cots')\n",
    "!pip3 uninstall -y openai\n",
    "!pip3 install openai==0.28 google-search-results wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Kw6dWX9WYl1",
    "outputId": "0b6f01e3-7967-447c-f432-40084cace9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 08:55:04.038053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 08:55:04.060226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 08:55:04.067248: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 08:55:04.083745: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 08:55:05.419007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/11\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/11\" 200 6690\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/11\" 200 6690\n",
      "True\n",
      "Generating predictions: 0it [00:00, ?it/s]DEBUG:filelock:Attempting to acquire lock 136812217120656 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/node_ip_address.json.lock\n",
      "DEBUG:filelock:Lock 136812217120656 acquired on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/node_ip_address.json.lock\n",
      "DEBUG:filelock:Attempting to release lock 136812217120656 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/node_ip_address.json.lock\n",
      "DEBUG:filelock:Lock 136812217120656 released on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/node_ip_address.json.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 136812217107120 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217107120 acquired on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to release lock 136812217107120 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217107120 released on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 136812217120272 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217120272 acquired on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to release lock 136812217120272 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217120272 released on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 136812217108224 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217108224 acquired on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to release lock 136812217108224 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217108224 released on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 136812217109280 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217109280 acquired on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to release lock 136812217109280 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217109280 released on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 136812217107312 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217107312 acquired on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Attempting to release lock 136812217107312 on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "DEBUG:filelock:Lock 136812217107312 released on /tmp/ray/session_2024-12-01_08-55-12_877795_17071/ports_by_node.json.lock\n",
      "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-01 08:55:15,300\tINFO worker.py:1819 -- Started a local Ray instance.\n",
      "\n",
      "Batches:   0% 0/364 [00:00<?, ?it/s]\u001b[A\n",
      "Batches:   0% 1/364 [00:00<05:21,  1.13it/s]\u001b[A\n",
      "Batches:   1% 2/364 [00:00<02:33,  2.35it/s]\u001b[A\n",
      "Batches:   1% 4/364 [00:01<01:16,  4.68it/s]\u001b[A\n",
      "Batches:   2% 6/364 [00:01<00:51,  6.90it/s]\u001b[A\n",
      "Batches:   2% 8/364 [00:01<00:40,  8.75it/s]\u001b[A\n",
      "Batches:   3% 10/364 [00:01<00:34, 10.38it/s]\u001b[A\n",
      "Batches:   3% 12/364 [00:01<00:29, 11.87it/s]\u001b[A\n",
      "Batches:   4% 14/364 [00:01<00:26, 13.39it/s]\u001b[A\n",
      "Batches:   5% 17/364 [00:01<00:20, 16.85it/s]\u001b[A\n",
      "Batches:   6% 21/364 [00:02<00:15, 21.70it/s]\u001b[A\n",
      "Batches:   7% 25/364 [00:02<00:13, 25.81it/s]\u001b[A\n",
      "Batches:   8% 30/364 [00:02<00:10, 30.90it/s]\u001b[A\n",
      "Batches:  10% 36/364 [00:02<00:08, 36.51it/s]\u001b[A\n",
      "Batches:  11% 41/364 [00:02<00:08, 38.25it/s]\u001b[A\n",
      "Batches:  13% 47/364 [00:02<00:07, 43.81it/s]\u001b[A\n",
      "Batches:  15% 53/364 [00:02<00:06, 48.08it/s]\u001b[A\n",
      "Batches:  16% 59/364 [00:02<00:06, 50.76it/s]\u001b[A\n",
      "Batches:  18% 66/364 [00:02<00:05, 55.61it/s]\u001b[A\n",
      "Batches:  20% 73/364 [00:03<00:04, 59.61it/s]\u001b[A\n",
      "Batches:  22% 80/364 [00:03<00:04, 61.62it/s]\u001b[A\n",
      "Batches:  24% 88/364 [00:03<00:04, 65.89it/s]\u001b[A\n",
      "Batches:  26% 96/364 [00:03<00:03, 69.91it/s]\u001b[A\n",
      "Batches:  29% 105/364 [00:03<00:03, 73.40it/s]\u001b[A\n",
      "Batches:  31% 113/364 [00:03<00:03, 74.94it/s]\u001b[A\n",
      "Batches:  34% 123/364 [00:03<00:03, 79.53it/s]\u001b[A\n",
      "Batches:  36% 132/364 [00:03<00:02, 82.03it/s]\u001b[A\n",
      "Batches:  39% 142/364 [00:03<00:02, 85.68it/s]\u001b[A\n",
      "Batches:  41% 151/364 [00:03<00:02, 85.89it/s]\u001b[A\n",
      "Batches:  44% 161/364 [00:04<00:02, 89.54it/s]\u001b[A\n",
      "Batches:  47% 172/364 [00:04<00:02, 92.42it/s]\u001b[A\n",
      "Batches:  50% 183/364 [00:04<00:01, 95.82it/s]\u001b[A\n",
      "Batches:  53% 194/364 [00:04<00:01, 99.43it/s]\u001b[A\n",
      "Batches:  57% 207/364 [00:04<00:01, 106.83it/s]\u001b[A\n",
      "Batches:  60% 219/364 [00:04<00:01, 109.99it/s]\u001b[A\n",
      "Batches:  64% 232/364 [00:04<00:01, 113.60it/s]\u001b[A\n",
      "Batches:  67% 245/364 [00:04<00:01, 118.05it/s]\u001b[A\n",
      "Batches:  71% 258/364 [00:04<00:00, 119.51it/s]\u001b[A\n",
      "Batches:  75% 272/364 [00:04<00:00, 124.60it/s]\u001b[A\n",
      "Batches:  79% 287/364 [00:05<00:00, 129.67it/s]\u001b[A\n",
      "Batches:  83% 302/364 [00:05<00:00, 133.66it/s]\u001b[A\n",
      "Batches:  87% 317/364 [00:05<00:00, 138.09it/s]\u001b[A\n",
      "Batches:  91% 332/364 [00:05<00:00, 141.52it/s]\u001b[A\n",
      "Batches:  95% 347/364 [00:05<00:00, 143.61it/s]\u001b[A\n",
      "Batches: 100% 364/364 [00:05<00:00, 64.72it/s] \n",
      "\n",
      "Batches: 100% 1/1 [00:00<00:00, 102.48it/s]\n",
      "Given the following question, answer it by providing follow up questions and intermediate answers. If intermediate questions are not necessarry, answe the question directly. You are provided with evidence that can help you arrive at the answer before the question.\n",
      "#\n",
      "Context1: The Big Red One: Fuller was a World War II veteran and served with the 1st Infantry Division, which is nicknamed \"The Big Red One\" for the red numeral \"1\" on the division's shoulder patch. He received the Silver Star, Bronze Star, and Purple Heart during his service.\n",
      "Question: how did the big red one get its name\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: its shoulder patch\n",
      "#\n",
      "Context1: Location Map of Cayman Islands: The given Cayman Islands location map shows that the Cayman Islands are located in the western Caribbean Sea. Location Map of Cayman Islands. Where is Cayman ...\n",
      "Question: where are the cayman islands on the map\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: western Caribbean Sea\n",
      "#\n",
      "Context1: Korean War | Combatants, Summary, Years, Map ... - Britannica: After more than a million combat casualties had been suffered on both sides, the fighting ended in July 1953 with Korea still divided into two hostile states. Negotiations in 1954 produced no further agreement, and the front line has been accepted ever since as the de facto boundary between North and South Korea.\n",
      "Question: who won the war between north korea and south korea\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: technically still at war\n",
      "#\n",
      "Context1: It's Always Sunny in Philadelphia (season 13): The thirteenth season of the American comedy television series It's Always Sunny in Philadelphia premiered on FXX on September 5, 2018. ... The season consists of ...\n",
      "Question: when does it's always sunny in philadelphia season 13 start\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: September 5, 2018\n",
      "#\n",
      "Context1: You've Got a Friend in Me: \"You've Got a Friend in Me\" is a song by Randy Newman. Used as the theme song for the 1995 Disney/Pixar animated film Toy Story, it has since become a major ...\n",
      "Question: who sang you got a friend in me from toy story\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: Randy Newman\n",
      "#\n",
      "Context1: April 1961: Yuri Gagarin from the Soviet Union was the first human in space. His vehicle, Vostok 1 circled Earth at a speed of 27,400 kilometers per hour with the flight lasting 108 minutes.\n",
      "Question: when was the first person sent to space\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: 12 April 1961\n",
      "#\n",
      "Context1: Power/Ability to: Manipulate the laws of physics.\n",
      "Question: what is a movie to feature a person who can create and control a device that can manipulate the laws of physics?\n",
      "Are follow up questions needed here: No.\n",
      "So the final answer is: \n",
      "\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:8000 \"POST /v1/completions HTTP/11\" 400 690\n",
      "INFO:root:got exception from fastchat server, sleeping...\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:8000 \"POST /v1/completions HTTP/11\" 400 690\n",
      "INFO:root:got exception from fastchat server, sleeping...\n",
      "Exception ignored in: <generator object tqdm.__iter__ at 0x7c6e167583c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1196, in __iter__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1302, in close\n",
      "    self.display(pos=0)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1495, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1151, in __str__\n",
      "    return self.format_meter(**self.format_dict)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 464, in format_meter\n",
      "    @staticmethod\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/content/cs245-project-crag/course_code/generate_retrobust.py\", line 393, in <module>\n",
      "    queries, ground_truths, predictions = generate_predictions(dataset_path, args.prompt_name, model, args.temperature, split)\n",
      "  File \"/content/cs245-project-crag/course_code/generate_retrobust.py\", line 291, in generate_predictions\n",
      "    model.call_gpt(full_prompt, '#', temp)\n",
      "  File \"/content/reasoning-on-cots/src/gpt3_accessors/gpt3_accessors/gpt_accessor_simple_retrobust.py\", line 30, in call_gpt\n",
      "    ans = inference_wrapper.complete(\n",
      "  File \"/content/reasoning-on-cots/src/inference/wrappers/fastchat.py\", line 61, in complete\n",
      "    sleep(10)\n",
      "KeyboardInterrupt\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python generate_retrobust.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --split 1 \\\n",
    "  --model_name \"retrobust_baseline\" \\\n",
    "  --llm_name \"meta-llama/Llama-2-13b-hf\" \\\n",
    "  --is_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0tG4btgmDfC",
    "outputId": "c987c4dd-d7c4-4340-b6a1-1928f78f4e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 21:06:54.587570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 21:06:54.609181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 21:06:54.615791: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 21:06:54.631869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 21:06:55.756041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "True\n",
      "  5% 65/1335 [25:50<8:24:57, 23.86s/it]Exception ignored in: <generator object tqdm.__iter__ at 0x7f7ee958e3b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1196, in __iter__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1302, in close\n",
      "    self.display(pos=0)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1495, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 459, in print_status\n",
      "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 452, in fp_write\n",
      "    fp.write(str(s))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\", line 196, in inner\n",
      "    return func(*args, **kwargs)\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/content/cs245-project-crag/course_code/evaluate.py\", line 130, in <module>\n",
      "    evaluation_results, llm_evaluation_logs = evaluate_predictions(results, eval_model)\n",
      "  File \"/content/cs245-project-crag/course_code/evaluate.py\", line 49, in evaluate_predictions\n",
      "    response = eval_model.evaluate(query, ground_truth, prediction)\n",
      "  File \"/content/cs245-project-crag/course_code/evaluation_model.py\", line 160, in evaluate\n",
      "    response = self.llm_client.chat.completions.create(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 991, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 926, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 954, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 991, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_client.py\", line 1027, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\", line 236, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\", line 106, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\", line 177, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\", line 217, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\", line 128, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python evaluate.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --model_name \"retrobust_baseline\" \\\n",
    "  --llm_name \"meta-llama/Llama-2-13b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G830pMi89vLj"
   },
   "source": [
    "# RetRobust++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41UOTrFi-UbX",
    "outputId": "d14800ff-8942-408a-a3bf-054afc488f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 0.28.0\n",
      "Uninstalling openai-0.28.0:\n",
      "  Successfully uninstalled openai-0.28.0\n",
      "Collecting openai\n",
      "  Using cached openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Using cached openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.55.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall -y openai\n",
    "!pip3 install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "w7SBeOtJYsZw"
   },
   "outputs": [],
   "source": [
    "# Kills existing service\n",
    "!pkill -f serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66Ey_HaC-dbS",
    "outputId": "33ad8434-0860-45dc-c1f6-dcb5e95aff31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2024-12-03 02:40:10.787230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 02:40:10.811750: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 02:40:10.818729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 02:40:10.836830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 02:40:11.968092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "config.json: 100% 951/951 [00:00<00:00, 4.95MB/s]\n",
      "WARNING 12-03 02:40:17 config.py:378] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-12-03 02:40:19,570\tINFO worker.py:1819 -- Started a local Ray instance.\n",
      "INFO 12-03 02:40:20 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='scoliono/retrobust_plusplus_combined_3b_f16', speculative_config=None, tokenizer='scoliono/retrobust_plusplus_combined_3b_f16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=scoliono/retrobust_plusplus_combined_3b_f16, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 43.5MB/s]\n",
      "tokenizer.json: 100% 17.2M/17.2M [00:00<00:00, 41.7MB/s]\n",
      "special_tokens_map.json: 100% 459/459 [00:00<00:00, 3.05MB/s]\n",
      "generation_config.json: 100% 171/171 [00:00<00:00, 1.11MB/s]\n",
      "INFO 12-03 02:40:24 ray_gpu_executor.py:134] use_ray_spmd_worker: False\n",
      "\u001b[36m(pid=480486)\u001b[0m 2024-12-03 02:40:27.826137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=480486)\u001b[0m 2024-12-03 02:40:27.853270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=480486)\u001b[0m 2024-12-03 02:40:27.860877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=480486)\u001b[0m 2024-12-03 02:40:29.147005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO 12-03 02:40:31 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-03 02:40:31 selector.py:116] Using XFormers backend.\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "INFO 12-03 02:40:32 model_runner.py:915] Starting to load model scoliono/retrobust_plusplus_combined_3b_f16...\n",
      "INFO 12-03 02:40:32 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-03 02:40:32 selector.py:116] Using XFormers backend.\n",
      "INFO 12-03 02:40:32 weight_utils.py:236] Using model weights format ['*.bin']\n",
      "pytorch_model-00001-of-00002.bin: 100% 4.97G/4.97G [00:16<00:00, 293MB/s]\n",
      "pytorch_model-00002-of-00002.bin: 100% 2.25G/2.25G [00:08<00:00, 252MB/s]\n",
      "Loading pt checkpoint shards:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/weight_utils.py:416: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards: 100% 2/2 [00:11<00:00,  5.79s/it]\n",
      "INFO 12-03 02:41:12 model_runner.py:926] Loading model weights took 6.0160 GB\n",
      "INFO 12-03 02:41:13 distributed_gpu_executor.py:57] # GPU blocks: 2568, # CPU blocks: 2340\n",
      "Generating predictions: 0it [00:00, ?it/s]\u001b[36m(_extract_chunks pid=480483)\u001b[0m 2024-12-03 02:41:28.507826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=480483)\u001b[0m 2024-12-03 02:41:28.548598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=480483)\u001b[0m 2024-12-03 02:41:28.561140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=480485)\u001b[0m 2024-12-03 02:41:28.619259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=480485)\u001b[0m 2024-12-03 02:41:28.659047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=480485)\u001b[0m 2024-12-03 02:41:28.673297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(_extract_chunks pid=480483)\u001b[0m 2024-12-03 02:41:30.681122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Generating predictions: 868it [18:38,  1.70s/it]\u001b[36m(_extract_chunks pid=480483)\u001b[0m /content/cs245-project-crag/course_code/retrobust_baseline.py:58: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\u001b[36m(_extract_chunks pid=480483)\u001b[0m   soup = BeautifulSoup(html_source, \"lxml\")\n",
      "\u001b[36m(_extract_chunks pid=481140)\u001b[0m 2024-12-03 02:41:28.963651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_extract_chunks pid=481140)\u001b[0m 2024-12-03 02:41:29.006979: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_extract_chunks pid=481140)\u001b[0m 2024-12-03 02:41:29.019279: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_extract_chunks pid=481140)\u001b[0m 2024-12-03 02:41:31.099233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Generating predictions: 1335it [28:54,  1.30s/it]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python generate_retrobust_plusplus.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --split 1 \\\n",
    "  --model_name \"retrobust_plusplus\" \\\n",
    "  --llm_name \"scoliono/retrobust_plusplus_combined_3b_f16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJtJidqX9zY7",
    "outputId": "b8407368-b190-404f-d452-0e5a5ebefb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Kills existing service\n",
    "!pkill -f serve\n",
    "# Use 'nohup' and & operator to start the inference server in the background\n",
    "#!nohup vllm serve scoliono/retrobust_plusplus_f16 --gpu_memory_utilization=0.95 --tensor_parallel_size=1 --dtype=\"half\" --port=8088 --enforce_eager &\n",
    "!nohup vllm serve unsloth/Llama-3.2-3B-Instruct-bnb-4bit --gpu_memory_utilization=0.95 --tensor_parallel_size=1 --dtype=\"half\" \\\n",
    "  --quantization=\"bitsandbytes\" --load-format=\"bitsandbytes\" --port=8088 --enforce_eager --max_model_len=4096 &\n",
    "# Give it time to load the model\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GYwSiIZI_Gc",
    "outputId": "c05ffd65-be43-4f4e-a0e4-5f51c9582012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-03 03:10:58.723496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 03:10:58.746009: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 03:10:58.752792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 03:10:58.768923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 03:11:00.016307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "True\n",
      "  0% 0/1335 [00:00<?, ?it/s]\u001b[32m2024-12-03 03:11:03.924\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 1, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:05.237\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 2, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:06.670\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 3, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:08.106\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 4, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:09.425\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 5, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:10.787\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 6, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:11.993\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 7, retrying...\u001b[0m\n",
      "\u001b[32m2024-12-03 03:11:13.368\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevaluation_model\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m177\u001b[0m - \u001b[33m\u001b[1mAPI call failed on attempt 8, retrying...\u001b[0m\n",
      "100% 1335/1335 [24:22<00:00,  1.10s/it]\n",
      "\u001b[32m2024-12-03 03:35:24.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_predictions\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m{'score': -0.0037453183520599342, 'exact_accuracy': 0.099625468164794, 'accuracy': 0.49812734082397003, 'hallucination': 0.50187265917603, 'missing': 0.0, 'n_miss': 0, 'n_correct': 665, 'n_correct_exact': 133, 'total': 1335}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd /content/cs245-project-crag/course_code && python evaluate.py \\\n",
    "  --dataset_path \"data/crag_task_1_dev_v4_release.jsonl.bz2\" \\\n",
    "  --model_name \"retrobust_plusplus\" \\\n",
    "  --llm_name \"scoliono/retrobust_plusplus_combined_3b_f16\" \\\n",
    "  --eval_llm_name \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" \\\n",
    "  --is_server \\\n",
    "  --vllm_server \"http://localhost:8088/v1\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nNY-a8M7WWO4"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
